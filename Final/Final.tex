\documentclass{article}

\usepackage[margin=.75in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[shortlabels]{enumitem}
\usepackage{float}
\usepackage{verbatim}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,calc}

\usepackage{mathtools}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%
\DeclarePairedDelimiter\abs{\lvert}{\rvert}%


\author{Damien Prieur}
\title{Final \\ MEM 633}
\date{}

\begin{document}
\maketitle

\section*{Problem 1}
Consider the following transfer function matrix
$$
H(s) = \frac{
    \begin{bmatrix}
        -s & s \\
        s+1 & 1 \\
    \end{bmatrix}
}{(s-1)(s+2)^2}
$$
\begin{enumerate}[(a)]
\item Find a state-space realization of $H(s)$ in block controller form.
\newline
Using the block controller form we can just follow the procedure.
$$ y(s) = N(s) D_R(s)^{-1}u(s) $$
$$
N(s) =
\begin{bmatrix}
-s & s \\
s+1 & 1 \\
\end{bmatrix}
\quad
D_R(s) =
(s-1)(s+2)^2
\begin{bmatrix}
1 & 0 \\
0 & 1 \\
\end{bmatrix}
$$
Let $\xi(s) = D_R(s)^{-1}u(s)$ then we get
$$ y(s) = N(s) \xi(s) \qquad D_R(s)\xi(s) = u(s) $$
Expanding $N(s)$ and $d(s)$we get
$$
N(s) =
\begin{bmatrix}
-1 & 1 \\
1 & 0 \\
\end{bmatrix}
s
+
\begin{bmatrix}
0 & 0 \\
1 & 1 \\
\end{bmatrix}
\qquad
d(s) = s^3 + 3s^2 - 4
$$
Let $N_2(s) = 0$, $N_1(s)$ be the first term of $N(s)$ without the $s$ and $N_0(s)$ be the last term of $N(s)$
\newline
Taking the inverse Laplace transform of $\xi(s)$ and $y(s)$ we get
$$
\dddot{\xi} =  - 3 \ddot{\xi} + 4\xi + U(t)
\qquad
y(t) =
\begin{bmatrix}
-1 & 1 \\
1 & 0 \\
\end{bmatrix}
\dot{\xi}
+
\begin{bmatrix}
0 & 0 \\
1 & 1 \\
\end{bmatrix}
\xi
$$
In matrix form we get
$$
\begin{bmatrix}
\dddot{\xi} \\
\ddot{\xi} \\
\dot{\xi} \\
\end{bmatrix}
=
\begin{bmatrix}
-3 & 0 & 4 \\
I & 0 & 0 \\
0 & I & 0 \\
\end{bmatrix}
\begin{bmatrix}
\ddot{\xi} \\
\dot{\xi} \\
\xi \\
\end{bmatrix}
+
\begin{bmatrix}
I \\
0 \\
0 \\
\end{bmatrix}
U(t)
$$
$$
y(t) =
\begin{bmatrix}
N_2 & N_1 & N_0
\end{bmatrix}
\begin{bmatrix}
\ddot{\xi} \\
\dot{\xi} \\
\xi \\
\end{bmatrix}
+
0 U(t)
$$
Now expanding $\xi = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}$ and plugging in we get
$$
\begin{bmatrix}
\dddot{x_1} \\
\dddot{x_2} \\
\ddot{x_1} \\
\ddot{x_2} \\
\dot{x_1} \\
\dot{x_2} \\
\end{bmatrix}
=
\begin{bmatrix}
-3 &  0 & 0 & 0 & 4 & 0 \\
 0 & -3 & 0 & 0 & 0 & 4 \\
 1 &  0 & 0 & 0 & 0 & 0 \\
 0 &  1 & 0 & 0 & 0 & 0 \\
 0 &  0 & 1 & 0 & 0 & 0 \\
 0 &  0 & 0 & 1 & 0 & 0 \\
\end{bmatrix}
\begin{bmatrix}
\ddot{x_1} \\
\ddot{x_2} \\
\dot{x_1} \\
\dot{x_2} \\
x_1 \\
x_2 \\
\end{bmatrix}
+
\begin{bmatrix}
1 & 0  \\
0 & 1  \\
0 & 0  \\
0 & 0  \\
0 & 0  \\
0 & 0  \\
\end{bmatrix}
\begin{bmatrix}
u_1 \\
u_2 \\
\end{bmatrix}
$$
$$
y(t) =
\begin{bmatrix}
0 & 0 & -1 & 1 & 0 & 0 \\
0 & 0 &  1 & 0 & 1 & 1 \\
\end{bmatrix}
\begin{bmatrix}
\ddot{x_1} \\
\ddot{x_2} \\
\dot{x_1} \\
\dot{x_2} \\
x_1 \\
x_2 \\
\end{bmatrix}
$$

\item Use the PBH Test to check the controllability and observability of the realization.
\newline
By the PBH test we know that $\{A,B\}$ is controllable if and only if $rank[sI-A \quad B] = n \quad \forall s$ where $n$ is the size of $A$
$$
\begin{bmatrix}
s+3 &   0 & 0 & 0 &-4 & 0 & 1 & 0\\
  0 & s+3 & 0 & 0 & 0 &-4 & 0 & 1\\
 -1 &   0 & s & 0 & 0 & 0 & 0 & 0\\
  0 &  -1 & 0 & s & 0 & 0 & 0 & 0\\
  0 &   0 &-1 & 0 & s & 0 & 0 & 0\\
  0 &   0 & 0 &-1 & 0 & s & 0 & 0\\
\end{bmatrix}
$$
We know that if $s$ is not an eigenvalue then the $A$ matrix is already going to be full rank, so we only need to check the eigenvalues of $A$
$$eign(A) = \{ -2, 1 \}$$
These eigenvalues are have multiplicity 4 and 2 respectively.
Plugging in these values we get
$$
s \to -2 =
\begin{bmatrix}
  1 &   0 & 0 & 0 &-4 & 0 & 1 & 0\\
  0 &   1 & 0 & 0 & 0 &-4 & 0 & 1\\
 -1 &   0 &-2 & 0 & 0 & 0 & 0 & 0\\
  0 &  -1 & 0 &-2 & 0 & 0 & 0 & 0\\
  0 &   0 &-1 & 0 &-2 & 0 & 0 & 0\\
  0 &   0 & 0 &-1 & 0 &-2 & 0 & 0\\
\end{bmatrix}
\quad
s \to 1 =
\begin{bmatrix}
  4 &   0 & 0 & 0 &-4 & 0 & 1 & 0\\
  0 &   4 & 0 & 0 & 0 &-4 & 0 & 1\\
 -1 &   0 & 1 & 0 & 0 & 0 & 0 & 0\\
  0 &  -1 & 0 & 1 & 0 & 0 & 0 & 0\\
  0 &   0 &-1 & 0 & 1 & 0 & 0 & 0\\
  0 &   0 & 0 &-1 & 0 & 1 & 0 & 0\\
\end{bmatrix}
$$
Which both have rank$= 6$ which is full rank.
\fbox{So the system is controllable.}
\newline
Now for observability from the PBH test we know that a system is controllable if and only if $rank\begin{bmatrix} sI-A \\ C \end{bmatrix} = n \quad \forall s$ where $n$ is the size of $A$
$$
\begin{bmatrix}
s+3 &   0 & 0 & 0 &-4 & 0 \\
  0 & s+3 & 0 & 0 & 0 &-4 \\
 -1 &   0 & s & 0 & 0 & 0 \\
  0 &  -1 & 0 & s & 0 & 0 \\
  0 &   0 &-1 & 0 & s & 0 \\
  0 &   0 & 0 &-1 & 0 & s \\
  0 &   0 &-1 & 1 & 0 & 0 \\
  0 &   0 & 1 & 0 & 1 & 1 \\
\end{bmatrix}
$$
Again we only need to look at the eigenvalues.
$$
s \to -2 =
\begin{bmatrix}
  1 &   0 & 0 & 0 &-4 & 0 \\
  0 &   1 & 0 & 0 & 0 &-4 \\
 -1 &   0 &-2 & 0 & 0 & 0 \\
  0 &  -1 & 0 &-2 & 0 & 0 \\
  0 &   0 &-1 & 0 &-2 & 0 \\
  0 &   0 & 0 &-1 & 0 &-2 \\
  0 &   0 &-1 & 1 & 0 & 0 \\
  0 &   0 & 1 & 0 & 1 & 1 \\
\end{bmatrix}
\quad
s \to 1 =
\begin{bmatrix}
  4 &   0 & 0 & 0 &-4 & 0 \\
  0 &   4 & 0 & 0 & 0 &-4 \\
 -1 &   0 & 1 & 0 & 0 & 0 \\
  0 &  -1 & 0 & 1 & 0 & 0 \\
  0 &   0 &-1 & 0 & 1 & 0 \\
  0 &   0 & 0 &-1 & 0 & 1 \\
  0 &   0 &-1 & 1 & 0 & 0 \\
  0 &   0 & 1 & 0 & 1 & 1 \\
\end{bmatrix}
$$
The first eigenvalue $-2$ only has rank $5$ while the second has rank $6$. \fbox{The system is not fully observable.}


\item Is it a minimal realization?
If not, find a similarity transformation to transform the realization into either controllability or observability decomposition.
Then find a minimal realization by eliminating the uncontrollable and/or unobservable parts of the system.
\newline
Since it's not both controllable and observable it's not a minimal realization.
Since we know that the observability is the issue we can perform observability decomposition to remove the unobservable parts of the system.
Following the process for observability decomposition we choose $T^{-1} = \begin{bmatrix}U_1 \\ U_2 \end{bmatrix}$ such that $U_1$ are $5$ linearly independent row vectors of the observability matrix and $U_2$ makes square matrix full rank.
If we take the first 5 rows of the observability matrix they are linearly independent.
$$
T^{-1} =
\begin{bmatrix}
-1 & 1 & 0 & 0 & 0 & 0 \\
1 & 0 & 1 & 1 & 0 & 0 \\
3 & -3 & 0 & 0 & -4 & 4 \\
-2 & 1 & 0 & 0 & 4 & 0 \\
-9 & 9 & -4 & 4 & 12 & -12 \\
1 & 0 & 0 & 0 & 0 & 0 \\
\end{bmatrix}
\implies
T =
\begin{bmatrix}
0 & 0 & 0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 & 0 & 1 \\
0 & \frac{1}{2} & -\frac{3}{8} & 0 & -\frac{1}{8} & -\frac{1}{2} \\
0 & \frac{1}{2} & \frac{3}{8} & 0 & \frac{1}{8} & -\frac{1}{2} \\
-\frac{1}{4} & 0 & 0 & \frac{1}{4} & 0 & \frac{1}{4} \\
\frac{1}{2} & 0 & \frac{1}{4} & \frac{1}{4} & 0 & \frac{1}{4} \\
\end{bmatrix}
$$
Applying the similarity transform $T$ as $\{\bar{A} = T^{-1}AT, \bar{B}=T^{-1}B, \bar{C}=CT\}$
$$
\bar{A} =
\begin{bmatrix}
0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 \\
1 & 2 & -\frac{1}{2} & -1 & -\frac{1}{2} & 0 \\
4 & 0 & 0 & 0 & -3 & 0 \\
-1 & 0 & 0 & 1 & 0 & -2 \\
\end{bmatrix}
\quad
\bar{B} =
\begin{bmatrix}
-1 & 1 \\
 1 & 0 \\
 3 & -3 \\
-2 & 1 \\
-9 & 9 \\
 1 & 0 \\
\end{bmatrix}
\quad
\bar{C} =
\begin{bmatrix}
0 & 0 & \frac{3}{4} & 0 & \frac{1}{4} & 0 \\
\frac{1}{4} & \frac{1}{2} & -\frac{1}{8} & \frac{1}{2} & -\frac{1}{8} & 0 \\
\end{bmatrix}
$$
If we take the $5x5$ subsystem and their corresponding terms in the $\bar{B}$ and $\bar{C}$ matricies we get an observable subsystem.
$$
\bar{A}_{min} =
\begin{bmatrix}
0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 1 \\
1 & 2 & -\frac{1}{2} & -1 & -\frac{1}{2} \\
4 & 0 & 0 & 0 & -3 \\
\end{bmatrix}
\quad
\bar{B}_{min} =
\begin{bmatrix}
-1 & 1 \\
 1 & 0 \\
 3 & -3 \\
-2 & 1 \\
-9 & 9 \\
\end{bmatrix}
\quad
\bar{C}_{min} =
\begin{bmatrix}
0 & 0 & \frac{3}{4} & 0 & \frac{1}{4} \\
\frac{1}{4} & \frac{1}{2} & -\frac{1}{8} & \frac{1}{2} & -\frac{1}{8} \\
\end{bmatrix}
$$
If we compute $\bar{C}_{min}\left(s I - \bar{A}_{min} \right)^{-1}\bar{B}_{min}$ we can verify that it gives the same transfer function we started with.

\item Determine the poles and zeros by using the $(A,B,C)$ matrices of the minimal realization in 1(c).
\newline
Note that the zeros of $(A,B,C)$ are the frequencies at which the rank of
$$
\begin{bmatrix}
sI-A & B \\
-C & D \\
\end{bmatrix}
$$
drops below its normal rank.
\newline
\newline
We can find the poles as the eigenvalues of the $A$ matrix.
$$
eig\left(
\begin{bmatrix}
0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 1 \\
1 & 2 & -\frac{1}{2} & -1 & -\frac{1}{2} \\
4 & 0 & 0 & 0 & -3 \\
\end{bmatrix}
\right)
=
\{-2, -2, -2, 1, 1\}
$$
For the poles we look at
$$
\begin{bmatrix}
sI-A & B \\
-C & D \\
\end{bmatrix}
=
\begin{bmatrix}
s & 0 & -1 & 0 & 0 & -1 & 1 \\
0 & s & 0 & -1 & 0 & 1 & 0 \\
0 & 0 & s & 0 & -1 & 3 & -3 \\
-1 & -2 & \frac{1}{2} & s+1 & \frac{1}{2} & -2 & 1 \\
-4 & 0 & 0 & 0 & s+3 & -9 & 9 \\
0 & 0 & -\frac{3}{4} & 0 & -\frac{1}{4} & 0 & 0 \\
-\frac{1}{4} & -\frac{1}{2} & \frac{1}{8} & -\frac{1}{2} & \frac{1}{8} & 0 & 0 \\
\end{bmatrix}
$$
We only need to look at when the determinant is zero to find where the matrix becomes rank deficient.
$$
det\left(
\begin{bmatrix}
s & 0 & -1 & 0 & 0 & -1 & 1 \\
0 & s & 0 & -1 & 0 & 1 & 0 \\
0 & 0 & s & 0 & -1 & 3 & -3 \\
-1 & -2 & \frac{1}{2} & s+1 & \frac{1}{2} & -2 & 1 \\
-4 & 0 & 0 & 0 & s+3 & -9 & 9 \\
0 & 0 & -\frac{3}{4} & 0 & -\frac{1}{4} & 0 & 0 \\
-\frac{1}{4} & -\frac{1}{2} & \frac{1}{8} & -\frac{1}{2} & \frac{1}{8} & 0 & 0 \\
\end{bmatrix}
\right)
=
-s
$$
So our only zero for the system is when $-s = 0$ or when $s=0$.
\newline
\fbox{The zeros of our system are $s = 0$ and the poles are $s = \{-2, -2, -2, 1, 1\}$}

\end{enumerate}

\newpage
\section*{Problem 2}
For the transfer matrix $H(s)$ shown in Problem \#1, which can be represented as
$$ H(s) = N(s)D(s)^{-1} $$
Where
$$
N(s) =
\begin{bmatrix}
-s & s \\
s+1 & 1 \\
\end{bmatrix}
,\quad
D(s) =
(s-1)(s+2)^2
\begin{bmatrix}
1 & 0 \\
0 & 1 \\
\end{bmatrix}
$$
\begin{enumerate}[(a)]
\item Find a greatest common right divisor (gcrd) of $N(s)$ and $D(s)$.
\newline
Constructing a gcrd by performing gaussian elimination on $ \begin{bmatrix} D(s) \\ N(s) \end{bmatrix} $
$$
\begin{bmatrix}
s^3 + 3s^2 -4 & 0 \\
0 & s^3 + 3s^2 -4 \\
-s & s \\
s+1 & 1 \\
\end{bmatrix}
$$
I performed these operations in this order
$$
u_1 =
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 1 & 1 \\
\end{bmatrix}
\quad
u_2 =
\begin{bmatrix}
0 & 0 & 0 & 1 \\
1 & 0 & 0 & -s^3-3 s^2+4 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & s \\
\end{bmatrix}
\quad
u_3 =
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 1 & -s-1 \\
0 & 1 & 0 & s^2+2 s-1 \\
\end{bmatrix}
\quad
u_4 =
\begin{bmatrix}
1 & 0 & \frac{1}{2} & 0 \\
0 & 0 & 1 & 0 \\
0 & 1 & \frac{s}{2} & 0 \\
0 & 0 & 1 & 1 \\
\end{bmatrix}
$$
And got the resulting matrix
$$
\begin{bmatrix}
1 & -1 \\
0 & -2 s-4 \\
0 & 0 \\
0 & 0 \\
\end{bmatrix}
$$
Taking the upper square matrix we get that our gcrd is
$$
gcrd =
\begin{bmatrix}
1 & -1 \\
0 & -2s-4 \\
\end{bmatrix}
$$

\item Find an irreducible right MFD for $H(s)$ by extracting the gcrd of $N(s)$ and $D(s)$.
\newline
To solve $N(s) = N_1(s)R(s)$ we can find the inverse of $R(s)$ and use $N(s)R^{-1}(s) = N_1(s)$
which we know exists since the matrix $\begin{bmatrix} D(s) \\ N(s) \end{bmatrix}$ has full column rank.
The same argument goes for $D_1(s)$
$$
R^{-1}(s) =
\begin{bmatrix}
1 & \frac{1}{-2s-4} \\
0 & \frac{1}{-2s-4} \\
\end{bmatrix}
\implies
N_1(s) =
\begin{bmatrix}
-s & 0 \\
s+1 & -\frac{1}{2} \\
\end{bmatrix}
\quad
D_1(s) =
(s-1)(s+2)
\begin{bmatrix}
s+2 & -\frac{1}{2} \\
0 & -\frac{1}{2} \\
\end{bmatrix}
$$
We can verify this holds true by plugging back into $H(s) = N(s)D(s)^{-1}$ and see that we get the original system.
We can also see that
$$ H(s) = N(s)D(s)^{-1} = N_1(s)R(s)\left(D_1(s)R(s)\right)^{-1} = N_1(s)D_1^{-1}(s) $$
This is our irreducible right MFD for $H(s)$
$$
H(s) =
\begin{bmatrix}
-s & 0 \\
s+1 & -\frac{1}{2} \\
\end{bmatrix}
\left(
(s-1)(s+2)
\begin{bmatrix}
s+2 & -\frac{1}{2} \\
0 & -\frac{1}{2} \\
\end{bmatrix}
\right)^{-1}
$$

\item Determine the poles and zeros of the system based on the irreducible MFD in (b).
\newline
Given a MFD the poles of $H(s)$ are the roots of the determinant of $D(s)$.
The zeros of a square non-singular $H(s)$ as we have are when frequencies when the rank of $N(s)$ drops below its normal rank $det(N(s)) = 0$
\newline
Starting with the poles we get
$$
(s-1)(s+2)
\begin{vmatrix}
s+2 & -\frac{1}{2} \\
0 & -\frac{1}{2} \\
\end{vmatrix}
=
-\frac{1}{2}(s-1)^2(s+2)^3
\implies s = \{-2,-2,-2,1,1\}
$$
We can see that the poles match our previous result in both value and multiplicity.
\newline
For the zeros we get
$$
\begin{vmatrix}
-s & 0 \\
s+1 & -\frac{1}{2} \\
\end{vmatrix}
= \frac{s}{2}
\implies s = 0
$$
Which again matches the zeros for our previous result.
\newline
\fbox{The zeros of our system are $s = 0$ and the poles are $s = \{-2, -2, -2, 1, 1\}$}

\item Find a state-space realization for the MFD in (c)
\newline
Finding the controller form realization from a right MFD we have
$$ A_c = A_C^o - B_C^oD_{hc}^{-1}D_{lc} \quad B_c = B_C^oD_{hc}^{-1} \quad C_c = N_{lc} $$
Where
\begin{itemize}[$\bullet$]
\item $A_C^o$ is block diagonal of ones below the diagonal with their number corresponding to how many derivatives are needed for each input.
\item $B_C^o$ is a column of all zeros except for a 1 corresponding to the start of the block it's associated with.
\item $D(s) = D_{hc}S(s) + D_{lc}(s)\Psi(s)$ where $S(s)$ is the largest degree $s$ in a column and $\Psi(s)$ is the remaining terms required for each column.
\item $N(s) = N_{lc}\Psi(s) $ where $\Psi(s)$ is the same as in the $D(s)$ equation.
\end{itemize}
Looking at $D(s)$ we can see that the first column is of degree $s^3$ while the second is has degree $s^2$.
So our realization will be of degree 5, and our $A_C^o$ and $B_C^o$ matrices are given by
$$
A_C^o =
\begin{bmatrix}
0 & 0 & 0 & 0 & 0 \\
1 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 \\
\end{bmatrix}
\quad
B_C^o =
\begin{bmatrix}
1 & 0 \\
0 & 0 \\
0 & 0 \\
0 & 1 \\
0 & 0 \\
\end{bmatrix}
$$
Our $S(s)$ and $\Psi(s)$ matrices are given by
$$
S(s) =
\begin{bmatrix}
s^3 & 0 \\
0 & s^2 \\
\end{bmatrix}
\quad
\Psi(s) =
\begin{bmatrix}
s^2 & 0 \\
s & 0 \\
1 & 0 \\
0 & s \\
0 & 1 \\
\end{bmatrix}
$$
With this we can find the remaining matrices
$$
D_{hc} =
\begin{bmatrix}
1 & -\frac{1}{2} \\
0 & -\frac{1}{2} \\
\end{bmatrix}
\quad
D_{lc} =
\begin{bmatrix}
3 & 0 & -4 & -\frac{1}{2} & 1 \\
0 & 0 &  0 & -\frac{1}{2} & 1 \\
\end{bmatrix}
\quad
N_{lc} =
\begin{bmatrix}
0 & -1 & 0 & 0 & 0 \\
0 & 1 & 1 & 0 & -\frac{1}{2} \\
\end{bmatrix}
$$
Plugging in and evaluating for $A_c, B_c, C_c$ we get
$$
A_c =
\begin{bmatrix}
-3 & 0 & 4 & 0 & 0 \\
1 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & -1 & 2 \\
0 & 0 & 0 & 1 & 0 \\
\end{bmatrix}
\quad
B_c =
\begin{bmatrix}
1 & -1 \\
0 & 0 \\
0 & 0 \\
0 & -2 \\
0 & 0 \\
\end{bmatrix}
\quad
C_c =
\begin{bmatrix}
0 & -1 & 0 & 0 & 0 \\
0 & 1 & 1 & 0 & -\frac{1}{2} \\
\end{bmatrix}
$$
Putting it all together our system looks like
$$
\begin{bmatrix}
\dddot{x}_1 \\
\ddot{x}_1 \\
\dot{x}_1 \\
\ddot{x}_2 \\
\dot{x}_2 \\
\end{bmatrix}
=
\begin{bmatrix}
-3 & 0 & 4 & 0 & 0 \\
1 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & -1 & 2 \\
0 & 0 & 0 & 1 & 0 \\
\end{bmatrix}
\begin{bmatrix}
\ddot{x}_1 \\
\dot{x}_1 \\
x_1 \\
\dot{x}_2 \\
x_2 \\
\end{bmatrix}
+
\begin{bmatrix}
1 & -1 \\
0 & 0 \\
0 & 0 \\
0 & -2 \\
0 & 0 \\
\end{bmatrix}
\begin{bmatrix}
u_1 \\
u_2 \\
\end{bmatrix}
$$
$$
y(t) =
\begin{bmatrix}
0 & -1 & 0 & 0 & 0 \\
0 & 1 & 1 & 0 & -\frac{1}{2} \\
\end{bmatrix}
\begin{bmatrix}
\ddot{x}_1 \\
\dot{x}_1 \\
x_1 \\
\dot{x}_2 \\
x_2 \\
\end{bmatrix}
$$

\item Is the state-space realization controllable and observable? Is it a minimal realization?
\newline
We know that this system is minimal since "any realization of an MFD with order equal to the degree of the determinant of the denominator matrix will be minimal if and only if the MFD is irreducible."
This is taken from the notes in chapter 4B page 28.
We can still verify it easily with the PBH Test.
\newline
We already have the eigenvalues so we just need to test the PBH controllability matrix for $s = \{-2,1\}$
$$
\begin{bmatrix}
s+3 & 0 & -4 & 0 & 0 & 1 & -1 \\
-1 & s & 0 & 0 & 0 & 0 & 0 \\
0 & -1 & s & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & s+1 & -2 & 0 & -2 \\
0 & 0 & 0 & -1 & s & 0 & 0 \\
\end{bmatrix}
$$
Evaluating and finding the rank for both we see that it does in fact has full rank of $5$ for both values.
\newline
Next the PBH observability matrix
$$
\begin{bmatrix}
s+3 & 0 & -4 & 0 & 0 \\
-1 & s & 0 & 0 & 0 \\
0 & -1 & s & 0 & 0 \\
0 & 0 & 0 & s+1 & -2 \\
0 & 0 & 0 & -1 & s \\
0 & -1 & 0 & 0 & 0 \\
0 & 1 & 1 & 0 & -\frac{1}{2} \\
\end{bmatrix}
$$
Evaluating and finding the rank for both we see that it again has a full rank of $5$ for both values.
\newline
\fbox{The system is both controllable, observable, and therefore minimal realization as we expect from an irreducible MFD}

\item Can you find a similarity transformation which relates the realization in Problem \#1(c) and problem \#2(d)?
If yes, show the results and procedure. If not explain.
\newline
We should be able to as they describe the same system and have the same degree.
To find the transformation we must find a matrix $T$ such that $\{A_2 = T^{-1}A_1T, B_2=T^{-1}B_1, C_2=C_1T\}$
Where $\{ A_1, B_1, C_1 \}$ represents the minimal representation from the first problem and $\{ A_1, B_1, C_1 \}$ from the second.
Matrices are similar if they both have the same diagonalization.
If we diagonailize both matrices
$$ A_1 = S_1JS_1^{-1} \quad A_2 = S_2JS_2^{-1} \implies S_1^{-1}A_1S_1 = J \quad S_2^{-1}A_2S_2 = J$$
Combining we get
$$ S_1^{-1}A_1S_1 = S_2^{-1}A_2S_2  \implies S_2S_1^{-1}A_1 = A_2S_2S_1^{-1}$$
$$ T = S_1^{-1}S_2 \implies A_2 = T^{-1}A_1T$$
So we just need to diagonalize by finding the Jordan decomposition since the matricies have repeated eigenvalues.
$$A_1 \to S_1 =
\begin{bmatrix}
0 & -\frac{1}{2} & -\frac{1}{4} & 1 & 0 \\
-\frac{1}{2} & \frac{1}{4} & \frac{1}{8} & 0 & 1 \\
0 & 1 & 0 & 1 & 0 \\
1 & -\frac{1}{2} & 0 & 0 & 1 \\
0 & -2 & 1 & 1 & 0 \\
\end{bmatrix}
\quad
J =
\begin{bmatrix}
-2 & 0 & 0 & 0 & 0 \\
0 & -2 & 1 & 0 & 0 \\
0 & 0 & -2 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 1 \\
\end{bmatrix}
$$
$$A_2 \to S_2 =
\begin{bmatrix}
0 & -4 & 4 & 0 & -1 \\
0 & 2 & -1 & 0 & -1 \\
0 & -1 & 0 & 0 & -1 \\
2 & 0 & 0 & -1 & 0 \\
-1 & 0 & 0 & -1 & 0 \\
\end{bmatrix}
\quad
J =
\begin{bmatrix}
-2 & 0 & 0 & 0 & 0 \\
0 & -2 & 1 & 0 & 0 \\
0 & 0 & -2 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 1 \\
\end{bmatrix}
$$
Which have the same Jordan decomposition as we expect.
Computing $T$ we get
$$
T =
S_2S_1^{-1}
=
\begin{bmatrix}
-\frac{11}{3} & -\frac{2}{3} & \frac{1}{2} & -\frac{1}{3} & \frac{19}{6} \\
\frac{1}{3} & -\frac{2}{3} & \frac{1}{2} & -\frac{1}{3} & -\frac{5}{6} \\
\frac{1}{3} & -\frac{2}{3} & -\frac{1}{2} & -\frac{1}{3} & \frac{1}{6} \\
-\frac{10}{9} & -\frac{4}{3} & \frac{2}{9} & \frac{4}{3} & -\frac{1}{9} \\
-\frac{1}{9} & \frac{2}{3} & -\frac{7}{9} & -\frac{2}{3} & -\frac{1}{9} \\
\end{bmatrix}
$$
Now we can verify that this similarity transformation does convert between the two realizations by performing the computation and checking.
This works for the $A$ matrix, but the transformation doesn't work for the $B$ and $C$ matrix.
Since the similarity Jordan decomposition matrices are unique there is only one transformation matrix for the $A$ matrix.
We can also see that if we apply the Jordan similarity transformation that $\bar{B}_1 \neq \bar{B}_2$ and $\bar{C}_1 \neq \bar{C}_2$ which also shows that there is not a similarity transformation.
\fbox{There is not a similarity transformation as our state space matrices $A$ are not diagonalizable}
\end{enumerate}

\newpage
\section*{Problem 3}
Consider the following system,
\begin{figure}[!htb]
\centering
\tikzset{
    block/.style = {draw, fill=white, rectangle, minimum height=1em, minimum width=1em},
    tmp/.style  = {coordinate, node distance=1cm},
    sum/.style= {draw, fill=white, circle, node distance=1cm},
    input/.style = {coordinate, node distance=1cm},
    output/.style= {coordinate, node distance=2cm},
    pinstyle/.style = {pin edge={to-,thin,black}},
}
\begin{tikzpicture}[auto, node distance=2cm,>=latex']
\node [sum, name=inputSum ](inputSum){};
\node [block, right of=inputSum](controller){$K(s)$};
\node [block, right of=controller](transferFunc){$G(s)$};
\node [tmp, right of=transferFunc](tmpOutput) {};
\node [output, right of=tmpOutput](output) {};
\node [tmp, below of=transferFunc](belowTF) {};

\draw [->] (inputSum) --  (controller) {};
\draw [->] (controller) -- node[below]{$u(t)$} (transferFunc) {};
\draw [-] (transferFunc) -- (tmpOutput) {};
\draw [->] (tmpOutput) -- node[above]{$Y(s)$} node[below]{$y(t)$} (output) {};
\draw [-] (tmpOutput) |- (belowTF) {};
\draw [->] (belowTF) -| (inputSum) {};
\end{tikzpicture}
\end{figure}
where
$$ G(s) = \frac{s-2}{s^2-4s} $$
In the following you will design an observer-based controller $K(s)$ so that the closed-loop system is stable.
\begin{enumerate}[(a)]
\item Find a state space representation of $G(s)$.
\newline
Using the controllable canonical form we get
$$
A =
\begin{bmatrix}
0 & 1 \\
0 & 4 \\
\end{bmatrix}
\quad
B =
\begin{bmatrix}
0 \\
1 \\
\end{bmatrix}
\quad
C =
\begin{bmatrix} -2 & 1 \end{bmatrix}
$$

\item Use the Riccati-equation approach to determine an observer based controller $K_1(s)$ such that the closed loop system is stable.
\newline
First we can start by describing the observer.
$$ \dot{\hat{x}} = A \hat{x} + Bu + H(-C\hat{x} -y) $$
Where $H$ is our observer gains.
Grouping we get
$$ \dot{\hat{x}} = (A - HC)\hat{x} + (B)u +Hy $$
Our stability is determined by $A-HC$.
Solving the ARE for our $H$ values we have
$$ H^T = -R^{-1}CX $$
Since the observer is the dual of the controller.
For the first choice of $R$ and $Q$ let them be the identity matrices.
This should put equal weight on all the parameter.
Without more insight to the system this should be a safe starting point.
Solving the Riccati equation we start with the Hamiltonian matrix with the transposed $A$ matrix since we are looking at the observer.
$$
H =
\begin{bmatrix}
A^T & -C^TR^{-1}C \\
-Q & -A \\
\end{bmatrix}
=
\begin{bmatrix}
0 & 0 & -4 & 2 \\
1 & 4 & 2 & -1 \\
-1 & 0 & 0 & -1 \\
0 & -1 & 0 & -4 \\
\end{bmatrix}
$$
The eigenvalues and eigenvectors of this matrix are
$$
\begin{bmatrix}
-\sqrt{17} \\
\sqrt{17} \\
-2 \\
2 \\
\end{bmatrix}
\quad
\begin{bmatrix}
-\frac{2}{13} \left(\sqrt{17}-2\right) & \sqrt{17}-4 & \frac{1}{13} \left(\sqrt{17}-2\right) & 1 \\
\frac{2}{13} \left(2+\sqrt{17}\right) & -4-\sqrt{17} & \frac{1}{13}
\left(-2-\sqrt{17}\right) & 1 \\
12 & -4 & 7 & 2 \\
-2 & 0 & 1 & 0 \\
\end{bmatrix}
$$
To construct $T$ we need the two eigenvectors that have eigenvalues with negative real components.
That is the second and fourth eigenvalues.
$$
T =
\begin{bmatrix}
-\frac{2}{13} \left(\sqrt{17}-2\right) & 12 \\
\sqrt{17}-4 & -4 \\
\frac{1}{13} \left(\sqrt{17}-2\right) & 7 \\
1 & 2 \\
\end{bmatrix}
\implies T_1 =
\begin{bmatrix}
-\frac{2}{13} \left(\sqrt{17}-2\right) & 12 \\
\sqrt{17}-4 & -4 \\
\end{bmatrix}
\quad T_2 =
\begin{bmatrix}
\frac{1}{13} \left(\sqrt{17}-2\right) & 7 \\
1 & 2 \\
\end{bmatrix}
$$
We can find the $X$ with $ X = T_2T_1^{-1} $
$$
X =
\begin{bmatrix}
\frac{19}{4}+\sqrt{17} & \frac{25}{2}+3 \sqrt{17} \\
\frac{25}{2}+3 \sqrt{17} & 37+9 \sqrt{17} \\
\end{bmatrix}
$$
With this we can find the observer gains.
$$ H^T = -R^{-1}CX =
\begin{bmatrix}
7.12311 & 24.3693 \\
\end{bmatrix}
$$
We can look at the eigenvalues of $A-HC$ and see that it's equal to the eigenvalues of the Hamiltonian matrix $-\sqrt{17}, -2$.
\newline
\newline
Then we have our state feedback based on our estimated state from the observer.
$$ u(t) = -F\hat{x} $$
Plugging in $u(t)$ into the first equation we get
$$ \dot{\hat{x}} = (A - HC)\hat{x} + (B)u +Hy $$
$$ \dot{\hat{x}} = (A - HC - BF)\hat{x} +Hy $$
Expanding we get the system as
$$ \dot{\hat{x}} = (A - BF - HC)\hat{x} +Hy $$
$$ u(t) = F\hat{x} $$
We want to find a stabilizing controller for the $A-BF$ system with the Riccati equation as well.
Following the same procedure as for the observer we have
$$
H =
\begin{bmatrix}
A & -BR^{-1}B^T \\
-Q & -A^T \\
\end{bmatrix}
=
\begin{bmatrix}
0 & 1 & 0 & 0 \\
0 & 4 & 0 & -1 \\
-1 & 0 & 0 & 0 \\
0 & -1 & -1 & -4 \\
\end{bmatrix}
$$
The eigenvalues and eigenvectors of this matrix are
$$
\begin{bmatrix}
-4.11594 \\
 4.11594 \\
-0.242958 \\
0.242958 \\
\end{bmatrix}
\quad
\begin{bmatrix}
-0.0299359 & 0.123214 & -0.00727315 & 1. \\
-2.09553 & -8.62507 & 0.509125 & 1. \\
-0.970064 & 0.235685 & -3.99273 & 1. \\
1.09553 & 0.266167 & -4.50912 & 1. \\
\end{bmatrix}
$$
To construct $T$ we need the two eigenvectors that have eigenvalues with negative real components.
That is the second and fourth eigenvalues.
$$
T =
\begin{bmatrix}
-0.0299359 & -0.970064 \\
0.123214 & 0.235685 \\
-0.00727315 & -3.99273 \\
1. & 1. \\
\end{bmatrix}
\implies T_1 =
\begin{bmatrix}
-0.0299359 & -0.970064 \\
0.123214 & 0.235685 \\
\end{bmatrix}
\quad T_2 =
\begin{bmatrix}
-0.00727315 & -3.99273 \\
1. & 1. \\
\end{bmatrix}
$$
We can find the $X$ with $ X = T_2T_1^{-1} $
$$
X =
\begin{bmatrix}
\sqrt{19} & 1 \\
1 & 4+\sqrt{19} \\
\end{bmatrix}
$$
With this we can find the observer gains.
$$ F = -R^{-1}B^TX =
=
\begin{bmatrix}
1 & 4+\sqrt{19} \\
\end{bmatrix}
$$
We can look at the eigenvalues of $A-BF$ and see that it's equal to the eigenvalues of the Hamiltonian matrix $-4.11594, -0.242958$.
\newline
Combining everything we've solved for we have
$$ \dot{\hat{x}} = (A - BF - HC)\hat{x} -Hy $$
$$ u(t) = F\hat{x} $$
Which we can rewrite into a more normal state space form with
$$
A_k = (A-BF-HC)
\quad
B_k = H
\quad
C_k = F
$$
With this we can write the transfer function for our controller with
$$ K_1(s) = C_k.(s I- A_k)^{-1}.B_k $$
$$ K_1(s) = \frac{\sqrt{17}-\left(51+13 \sqrt{17}+3 \sqrt{19} \left(4+\sqrt{17}\right)\right) s}{-s\left(s+\sqrt{19}+\sqrt{17}+6\right)+2 \sqrt{19} \left(3+\sqrt{17}\right)+7 \sqrt{17}+26}$$
Or numerically
$$ K_1(s) \approx \frac{- 210.824 s + 4.12311 }{-s^2-14.482 s+116.96} $$

\item Find a state space representation of the closed loop system with $y(t)$ as the output.
\newline
Let our state variables be $x(t)$ and $\hat{x}(t)$.
$$
\begin{bmatrix}
\dot{x} \\
\dot{\hat{x}} \\
\end{bmatrix}
=
\begin{bmatrix}
A & -BF \\
B_kC & A_k \\
\end{bmatrix}
\begin{bmatrix}
x\\
\hat{x} \\
\end{bmatrix}
$$
Plugging in terms
$$
\begin{bmatrix}
\dot{x}_1 \\
\dot{x}_2 \\
\dot{\hat{x}}_1 \\
\dot{\hat{x}}_2 \\
\end{bmatrix}
=
\begin{bmatrix}
0. & 1. & 0. & 0. \\
0. & 4. & -1. & -8.3589 \\
14.2462 & -7.12311 & 14.2462 & -6.12311 \\
48.7386 & -24.3693 & 47.7386 & -28.7282 \\
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \\
\hat{x}_1 \\
\hat{x}_2 \\
\end{bmatrix}
$$
With the output
$$
y(t) =
\begin{bmatrix}
-2 & 1 & 0 & 0
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \\
\hat{x}_1 \\
\hat{x}_2 \\
\end{bmatrix}
$$
We can find the closed form solution by taking the matrix exponential of the combined state space matrix $A$

\item Verify that the closed loop system poles are the regulator poles together with the observer poles.
\newline
Looking at the 4x4 matrix in our closed loop we can check the eigenvalues and we expect them to match the poles chosen during the Riccati equation solving step.
$$ eig(A_{cl})= \{-4.12311, -4.11594, -2., -0.242958\} $$
The ones chosen for the observer poles were $\{ -4.12311, -2\}$ which are there.
The eigenvalues chosen for the controller were $\{-4.11594,-0.242958\}$ which are the remaining two eigenvalues.
We also know this will be true due to the separation principle since we can write the closed loop dynamics as
$$
\begin{bmatrix} \dot{x} \\ \dot{e} \end{bmatrix} =
\begin{bmatrix}
A - BF & BF \\
0 & A - HC \\
\end{bmatrix}
\begin{bmatrix} x \\ e \end{bmatrix}
$$
Where $e = x -\hat{x}$. This system shows that the eigenvalues of the closes system are those of the diagonal entries separately as it's an upper triangular matrix.

\newpage
\item Plot the state response for $x(t)$ and $u(t)$ due to the initial conditions $y(0)=1$, and $\dot{y}(0)=0$.
\newline
Since the derivatives of both $x_1$ and $x_2$ are only dependent on $x_2$ it must be zero due to the initial condition $\dot{y}(0)=0$.
So $y(0)=1 \implies x_1 = -\frac{1}{2}$.
\begin{figure}[!htb]
\centering
\includegraphics[scale=.5]{images/p3_cl1_states.png}
\includegraphics[scale=.5]{images/p3_cl1_input.png}
\end{figure}

\item Repeat (b), (c), (d), and (e) with a different observer-based controller $K_2(s)$, which is obtained by choosing different weighting matrices in the Riccati equations.
\newline
\begin{enumerate}[(a)]
\setcounter{enumii}{1}
\item Before we chose the identity for all weighting matricies.
This time we can give preferential weights to certain terms.
$$
R =
\begin{bmatrix} 1 \end{bmatrix}
Q =
\begin{bmatrix} 100 & 0 \\ 0 & 10 \end{bmatrix}
$$
This should put a lot of emphasis on the $x_1$ term and try to correct that as soon as possible.
With $R$ beign the smallest it should not penalize the use of the input very much so can expect larger use of our input.
Beginning with our observer we get
$$
H =
\begin{bmatrix}
0 & 0 & -4 & 2 \\
1 & 4 & 2 & -1 \\
-100 & 0 & 0 & -1 \\
0 & -10 & 0 & -4 \\
\end{bmatrix}
\implies
T =
\begin{bmatrix}
-3.54336 & 0.0102095 \\
1.6256 & -0.00382243 \\
-17.4435 & 0.510112 \\
1. & 1. \\
\end{bmatrix}
\quad \lambda_i = \{-20.256, -3.9618\}
$$
$$
X =
\begin{bmatrix}
249.833 & 533.838 \\
 533.838 & 1164.24 \\
\end{bmatrix}
\implies
H^T = \begin{bmatrix} 34.1713 & 96.5603 \end{bmatrix}
$$
Which has the eigenvalues from the $H$ matrix as we expect.
\newline
Now for the controller
$$
H =
\begin{bmatrix}
0 & 1 & 0 & 0 \\
0 & 4 & 0 & -1 \\
-100 & 0 & 0 & 0 \\
0 & -10 & -1 & -4 \\
\end{bmatrix}
\implies
T =
\begin{bmatrix}
-0.0251444 & -0.0748556 \\
0.116064 & 0.162169 \\
-0.544734 & -3.45527 \\
1. & 1. \\
\end{bmatrix}
\quad \lambda_i = \{-4.61591, -2.16642\}
$$
$$
X =
\begin{bmatrix}
67.8233 & 10. \\
10. & 10.7823 \\
\end{bmatrix}
\implies
F = \begin{bmatrix} 10 & 10.7823 \end{bmatrix}
$$
Which again has the eigenvalues from $H$ that we expect.
We can now create our $A_k$ $B_k$ and $C_k$ matricies as before and compute our transfer function $K_2(s)$
$$ K_2(s) = \frac{1382.86 s-401.248}{s^2+35.0001 s-988.355} $$

\item Finding a state space equation as we did before
$$
\begin{bmatrix}
\dot{x} \\
\dot{\hat{x}} \\
\end{bmatrix}
=
\begin{bmatrix}
A & -BF \\
B_kC & A_k \\
\end{bmatrix}
\begin{bmatrix}
x\\
\hat{x} \\
\end{bmatrix}
$$
Plugging in terms gives us
$$
\begin{bmatrix}
\dot{x}_1 \\
\dot{x}_2 \\
\dot{\hat{x}}_1 \\
\dot{\hat{x}}_2 \\
\end{bmatrix}
=
\begin{bmatrix}
0. & 1. & 0. & 0. \\
0. & 4. & -10. & -10.7823 \\
-68.3426 & 34.1713 & 68.3426 & -33.1713 \\
-193.121 & 96.5603 & 183.121 & -103.343 \\
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \\
\hat{x}_1 \\
\hat{x}_2 \\
\end{bmatrix}
$$
With the output
$$
y(t) =
\begin{bmatrix}
-2 & 1 & 0 & 0
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \\
\hat{x}_1 \\
\hat{x}_2 \\
\end{bmatrix}
$$

\item Finding the eigenvalues of our closed loop system's $A$ matrix we get
$$ eig(A) = \{-20.256, -4.61591, -3.96178, -2.16642\} $$
Which do match the poles chosen by the Riccati equation for the observer $\{-20.256, -3.9618\}$ and the controller $\{-4.61591, -2.16642\}$

\item On the next page
\begin{figure}[!htb]
\centering
\includegraphics[scale=.5]{images/p3_cl2_states.png}
\includegraphics[scale=.5]{images/p3_cl2_input.png}
\end{figure}

\end{enumerate}

\newpage
\item Comment on how the weighting matrices in the Riccati equation and the pole location of the closed-loop system affect the closed loop system performance.
\newline
As expected by decreasing the relative weight of $R$ it allowed the system to apply more control input to they system.
We can see that the system did apply a large force multiple times compared to the one spike to 20.
The significantly higher weighting on $x_1$ causes the maximum deviation of it to be much smaller, $2$ vs $5$.
It's also important to note that this system setteled much faster, in about 3.5 seconds while the first system was still not at zero after 20 seconds.
The observer values also converged much quicker due to the larger terms as it was able to change it's value quicker (input) and wanted to minimize the $x_1$ error the most.
\newline
\newline
Another intersting example is to swap the $x_1$ and $x_2$ weights in $Q$ as $x_1 = \dot{x}_2$.
So if we try to keep $x_2$ small we are limiting the rate at which $x_1$ can reach its target value.
We would expect the $x_2$ value to be close to zero while the $x_1$ value takes a long time to go to zero.
Which is exactly what we see.
\begin{figure}[!htb]
\centering
\includegraphics[scale=.35]{images/p3_extra_states.png}
\end{figure}

\end{enumerate}

\end{document}

\documentclass{article}

\usepackage[margin=.75in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[shortlabels]{enumitem}
\usepackage{float}
\usepackage{verbatim}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,calc}

\usepackage{mathtools}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%
\DeclarePairedDelimiter\abs{\lvert}{\rvert}%


\author{Damien Prieur}
\title{Final \\ MEM 633}
\date{}

\begin{document}
\maketitle

\section*{Problem 1}
Consider the following transfer function matrix
$$
H(s) = \frac{
    \begin{bmatrix}
        -s & s \\
        s+1 & 1 \\
    \end{bmatrix}
}{(s-1)(s+2)^2}
$$
\begin{enumerate}[(a)]
\item Find a state-space realization of $H(s)$ in block controller form.
\newline
Using the block controller form we can just follow the procedure.
$$ y(s) = N(s) D_R(s)^{-1}u(s) $$
$$
N(s) =
\begin{bmatrix}
-s & s \\
s+1 & 1 \\
\end{bmatrix}
\quad
D_R(s) =
(s-1)(s+2)^2
\begin{bmatrix}
1 & 0 \\
0 & 1 \\
\end{bmatrix}
$$
Let $\xi(s) = D_R(s)^{-1}u(s)$ then we get
$$ y(s) = N(s) \xi(s) \qquad D_R(s)\xi(s) = u(s) $$
Expanding $N(s)$ and $d(s)$we get
$$
N(s) =
\begin{bmatrix}
-1 & 1 \\
1 & 0 \\
\end{bmatrix}
s
+
\begin{bmatrix}
0 & 0 \\
1 & 1 \\
\end{bmatrix}
\qquad
d(s) = s^3 + 3s^2 - 4
$$
Let $N_2(s) = 0$, $N_1(s)$ be the first term of $N(s)$ without the $s$ and $N_0(s)$ be the last term of $N(s)$
\newline
Taking the inverse Laplace transform of $\xi(s)$ and $y(s)$ we get
$$
\dddot{\xi} =  - 3 \ddot{\xi} + 4\xi + U(t)
\qquad
y(t) =
\begin{bmatrix}
-1 & 1 \\
1 & 0 \\
\end{bmatrix}
\dot{\xi}
+
\begin{bmatrix}
0 & 0 \\
1 & 1 \\
\end{bmatrix}
\xi
$$
In matrix form we get
$$
\begin{bmatrix}
\dddot{\xi} \\
\ddot{\xi} \\
\dot{\xi} \\
\end{bmatrix}
=
\begin{bmatrix}
-3 & 0 & 4 \\
I & 0 & 0 \\
0 & I & 0 \\
\end{bmatrix}
\begin{bmatrix}
\ddot{\xi} \\
\dot{\xi} \\
\xi \\
\end{bmatrix}
+
\begin{bmatrix}
I \\
0 \\
0 \\
\end{bmatrix}
U(t)
$$
$$
y(t) =
\begin{bmatrix}
N_2 & N_1 & N_0
\end{bmatrix}
\begin{bmatrix}
\ddot{\xi} \\
\dot{\xi} \\
\xi \\
\end{bmatrix}
+
0 U(t)
$$
Now expanding $\xi = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}$ and plugging in we get
$$
\begin{bmatrix}
\dddot{x_1} \\
\dddot{x_2} \\
\ddot{x_1} \\
\ddot{x_2} \\
\dot{x_1} \\
\dot{x_2} \\
\end{bmatrix}
=
\begin{bmatrix}
-3 &  0 & 0 & 0 & 4 & 0 \\
 0 & -3 & 0 & 0 & 0 & 4 \\
 1 &  0 & 0 & 0 & 0 & 0 \\
 0 &  1 & 0 & 0 & 0 & 0 \\
 0 &  0 & 1 & 0 & 0 & 0 \\
 0 &  0 & 0 & 1 & 0 & 0 \\
\end{bmatrix}
\begin{bmatrix}
\ddot{x_1} \\
\ddot{x_2} \\
\dot{x_1} \\
\dot{x_2} \\
x_1 \\
x_2 \\
\end{bmatrix}
+
\begin{bmatrix}
1 & 0  \\
0 & 1  \\
0 & 0  \\
0 & 0  \\
0 & 0  \\
0 & 0  \\
\end{bmatrix}
\begin{bmatrix}
u_1 \\
u_2 \\
\end{bmatrix}
$$
$$
y(t) =
\begin{bmatrix}
0 & 0 & -1 & 1 & 0 & 0 \\
0 & 0 &  1 & 0 & 1 & 1 \\
\end{bmatrix}
\begin{bmatrix}
\ddot{x_1} \\
\ddot{x_2} \\
\dot{x_1} \\
\dot{x_2} \\
x_1 \\
x_2 \\
\end{bmatrix}
$$

\item Use the PBH Test to check the controllability and observability of the realization.
\newline
By the PBH test we know that $\{A,B\}$ is controllable if and only if $rank[sI-A \quad B] = n \quad \forall s$ where $n$ is the size of $A$
$$
\begin{bmatrix}
s+3 &   0 & 0 & 0 &-4 & 0 & 1 & 0\\
  0 & s+3 & 0 & 0 & 0 &-4 & 0 & 1\\
 -1 &   0 & s & 0 & 0 & 0 & 0 & 0\\
  0 &  -1 & 0 & s & 0 & 0 & 0 & 0\\
  0 &   0 &-1 & 0 & s & 0 & 0 & 0\\
  0 &   0 & 0 &-1 & 0 & s & 0 & 0\\
\end{bmatrix}
$$
We know that if $s$ is not an eigenvalue then the $A$ matrix is already going to be full rank, so we only need to check the eigenvalues of $A$
$$eign(A) = \{ -2, 1 \}$$
These eigenvalues are have multiplicity 4 and 2 respectively.
Plugging in these values we get
$$
s \to -2 =
\begin{bmatrix}
  1 &   0 & 0 & 0 &-4 & 0 & 1 & 0\\
  0 &   1 & 0 & 0 & 0 &-4 & 0 & 1\\
 -1 &   0 &-2 & 0 & 0 & 0 & 0 & 0\\
  0 &  -1 & 0 &-2 & 0 & 0 & 0 & 0\\
  0 &   0 &-1 & 0 &-2 & 0 & 0 & 0\\
  0 &   0 & 0 &-1 & 0 &-2 & 0 & 0\\
\end{bmatrix}
\quad
s \to 1 =
\begin{bmatrix}
  4 &   0 & 0 & 0 &-4 & 0 & 1 & 0\\
  0 &   4 & 0 & 0 & 0 &-4 & 0 & 1\\
 -1 &   0 & 1 & 0 & 0 & 0 & 0 & 0\\
  0 &  -1 & 0 & 1 & 0 & 0 & 0 & 0\\
  0 &   0 &-1 & 0 & 1 & 0 & 0 & 0\\
  0 &   0 & 0 &-1 & 0 & 1 & 0 & 0\\
\end{bmatrix}
$$
Which both have rank$= 6$ which is full rank.
\fbox{So the system is controllable.}
\newline
Now for observability from the PBH test we know that a system is controllable if and only if $rank\begin{bmatrix} sI-A \\ C \end{bmatrix} = n \quad \forall s$ where $n$ is the size of $A$
$$
\begin{bmatrix}
s+3 &   0 & 0 & 0 &-4 & 0 \\
  0 & s+3 & 0 & 0 & 0 &-4 \\
 -1 &   0 & s & 0 & 0 & 0 \\
  0 &  -1 & 0 & s & 0 & 0 \\
  0 &   0 &-1 & 0 & s & 0 \\
  0 &   0 & 0 &-1 & 0 & s \\
  0 &   0 &-1 & 1 & 0 & 0 \\
  0 &   0 & 1 & 0 & 1 & 1 \\
\end{bmatrix}
$$
Again we only need to look at the eigenvalues.
$$
s \to -2 =
\begin{bmatrix}
  1 &   0 & 0 & 0 &-4 & 0 \\
  0 &   1 & 0 & 0 & 0 &-4 \\
 -1 &   0 &-2 & 0 & 0 & 0 \\
  0 &  -1 & 0 &-2 & 0 & 0 \\
  0 &   0 &-1 & 0 &-2 & 0 \\
  0 &   0 & 0 &-1 & 0 &-2 \\
  0 &   0 &-1 & 1 & 0 & 0 \\
  0 &   0 & 1 & 0 & 1 & 1 \\
\end{bmatrix}
\quad
s \to 1 =
\begin{bmatrix}
  4 &   0 & 0 & 0 &-4 & 0 \\
  0 &   4 & 0 & 0 & 0 &-4 \\
 -1 &   0 & 1 & 0 & 0 & 0 \\
  0 &  -1 & 0 & 1 & 0 & 0 \\
  0 &   0 &-1 & 0 & 1 & 0 \\
  0 &   0 & 0 &-1 & 0 & 1 \\
  0 &   0 &-1 & 1 & 0 & 0 \\
  0 &   0 & 1 & 0 & 1 & 1 \\
\end{bmatrix}
$$
The first eigenvalue $-2$ only has rank $5$ while the second has rank $6$. \fbox{The system is not fully observable.}


\item Is it a minimal realization?
If not, find a similarity transformation to transform the realization into either controllability or observability decomposition.
Then find a minimal realization by eliminating the uncontrollable and/or unobservable parts of the system.
\newline
Since it's not both controllable and observable it's not a minimal realization.
Since we know that the observability is the issue we can perform observability decomposition to remove the unobservable parts of the system.
Following the process for observability decomposition we choose $T^{-1} = \begin{bmatrix}U_1 \\ U_2 \end{bmatrix}$ such that $U_1$ are $5$ linearly independent row vectors of the observability matrix and $U_2$ makes square matrix full rank.
If we take the first 5 rows of the observability matrix they are linearly independent.
$$
T^{-1} =
\begin{bmatrix}
-1 & 1 & 0 & 0 & 0 & 0 \\
1 & 0 & 1 & 1 & 0 & 0 \\
3 & -3 & 0 & 0 & -4 & 4 \\
-2 & 1 & 0 & 0 & 4 & 0 \\
-9 & 9 & -4 & 4 & 12 & -12 \\
1 & 0 & 0 & 0 & 0 & 0 \\
\end{bmatrix}
\implies
T =
\begin{bmatrix}
0 & 0 & 0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 & 0 & 1 \\
0 & \frac{1}{2} & -\frac{3}{8} & 0 & -\frac{1}{8} & -\frac{1}{2} \\
0 & \frac{1}{2} & \frac{3}{8} & 0 & \frac{1}{8} & -\frac{1}{2} \\
-\frac{1}{4} & 0 & 0 & \frac{1}{4} & 0 & \frac{1}{4} \\
\frac{1}{2} & 0 & \frac{1}{4} & \frac{1}{4} & 0 & \frac{1}{4} \\
\end{bmatrix}
$$
Applying the similarity transform $T$ as $\{\bar{A} = T^{-1}AT, \bar{B}=T^{-1}B, \bar{C}=CT\}$
$$
\bar{A} =
\begin{bmatrix}
0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 \\
1 & 2 & -\frac{1}{2} & -1 & -\frac{1}{2} & 0 \\
4 & 0 & 0 & 0 & -3 & 0 \\
-1 & 0 & 0 & 1 & 0 & -2 \\
\end{bmatrix}
\quad
\bar{B} =
\begin{bmatrix}
-1 & 1 \\
 1 & 0 \\
 3 & -3 \\
-2 & 1 \\
-9 & 9 \\
 1 & 0 \\
\end{bmatrix}
\quad
\bar{C} =
\begin{bmatrix}
0 & 0 & \frac{3}{4} & 0 & \frac{1}{4} & 0 \\
\frac{1}{4} & \frac{1}{2} & -\frac{1}{8} & \frac{1}{2} & -\frac{1}{8} & 0 \\
\end{bmatrix}
$$
If we take the $5x5$ subsystem and their corresponding terms in the $\bar{B}$ and $\bar{C}$ matricies we get an observable subsystem.
$$
\bar{A}_{min} =
\begin{bmatrix}
0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 1 \\
1 & 2 & -\frac{1}{2} & -1 & -\frac{1}{2} \\
4 & 0 & 0 & 0 & -3 \\
\end{bmatrix}
\quad
\bar{B}_{min} =
\begin{bmatrix}
-1 & 1 \\
 1 & 0 \\
 3 & -3 \\
-2 & 1 \\
-9 & 9 \\
\end{bmatrix}
\quad
\bar{C}_{min} =
\begin{bmatrix}
0 & 0 & \frac{3}{4} & 0 & \frac{1}{4} \\
\frac{1}{4} & \frac{1}{2} & -\frac{1}{8} & \frac{1}{2} & -\frac{1}{8} \\
\end{bmatrix}
$$
If we compute $\bar{C}_{min}\left(s I - \bar{A}_{min} \right)^{-1}\bar{B}_{min}$ we can verify that it gives the same transfer function we started with.

\item Determine the poles and zeros by using the $(A,B,C)$ matrices of the minimal realization in 1(c).
\newline
Note that the zeros of $(A,B,C)$ are the frequencies at which the rank of
$$
\begin{bmatrix}
sI-A & B \\
-C & D \\
\end{bmatrix}
$$
drops below its normal rank.
\newline
\newline
We can find the poles as the eigenvalues of the $A$ matrix.
$$
eig\left(
\begin{bmatrix}
0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 1 \\
1 & 2 & -\frac{1}{2} & -1 & -\frac{1}{2} \\
4 & 0 & 0 & 0 & -3 \\
\end{bmatrix}
\right)
=
\{-2, -2, -2, 1, 1\}
$$
For the poles we look at
$$
\begin{bmatrix}
sI-A & B \\
-C & D \\
\end{bmatrix}
=
\begin{bmatrix}
s & 0 & -1 & 0 & 0 & -1 & 1 \\
0 & s & 0 & -1 & 0 & 1 & 0 \\
0 & 0 & s & 0 & -1 & 3 & -3 \\
-1 & -2 & \frac{1}{2} & s+1 & \frac{1}{2} & -2 & 1 \\
-4 & 0 & 0 & 0 & s+3 & -9 & 9 \\
0 & 0 & -\frac{3}{4} & 0 & -\frac{1}{4} & 0 & 0 \\
-\frac{1}{4} & -\frac{1}{2} & \frac{1}{8} & -\frac{1}{2} & \frac{1}{8} & 0 & 0 \\
\end{bmatrix}
$$
We only need to look at when the determinant is zero to find where the matrix becomes rank deficient.
$$
det\left(
\begin{bmatrix}
s & 0 & -1 & 0 & 0 & -1 & 1 \\
0 & s & 0 & -1 & 0 & 1 & 0 \\
0 & 0 & s & 0 & -1 & 3 & -3 \\
-1 & -2 & \frac{1}{2} & s+1 & \frac{1}{2} & -2 & 1 \\
-4 & 0 & 0 & 0 & s+3 & -9 & 9 \\
0 & 0 & -\frac{3}{4} & 0 & -\frac{1}{4} & 0 & 0 \\
-\frac{1}{4} & -\frac{1}{2} & \frac{1}{8} & -\frac{1}{2} & \frac{1}{8} & 0 & 0 \\
\end{bmatrix}
\right)
=
-s
$$
So our only zero for the system is when $-s = 0$ or when $s=0$.
\newline
\fbox{The zeros of our system are $s = 0$ and the poles are $s = \{-2, -2, -2, 1, 1\}$}

\end{enumerate}

\newpage
\section*{Problem 2}
For the transfer matrix $H(s)$ shown in Problem \#1, which can be represented as
$$ H(s) = N(s)D(s)^{-1} $$
Where
$$
N(s) =
\begin{bmatrix}
-s & s \\
s+1 & 1 \\
\end{bmatrix}
,\quad
D(s) =
(s-1)(s+2)^2
\begin{bmatrix}
1 & 0 \\
0 & 1 \\
\end{bmatrix}
$$
\begin{enumerate}[(a)]
\item Find a greatest common right divisor (gcrd) of $N(s)$ and $D(s)$.
\newline
Constructing a gcrd by performing gaussian elimination on $ \begin{bmatrix} D(s) \\ N(s) \end{bmatrix} $
$$
\begin{bmatrix}
s^3 + 3s^2 -4 & 0 \\
0 & s^3 + 3s^2 -4 \\
-s & s \\
s+1 & 1 \\
\end{bmatrix}
$$
I performed these operations in this order
$$
u_1 =
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 1 & 1 \\
\end{bmatrix}
\quad
u_2 =
\begin{bmatrix}
0 & 0 & 0 & 1 \\
1 & 0 & 0 & -s^3-3 s^2+4 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & s \\
\end{bmatrix}
\quad
u_3 =
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 1 & -s-1 \\
0 & 1 & 0 & s^2+2 s-1 \\
\end{bmatrix}
\quad
u_4 =
\begin{bmatrix}
1 & 0 & \frac{1}{2} & 0 \\
0 & 0 & 1 & 0 \\
0 & 1 & \frac{s}{2} & 0 \\
0 & 0 & 1 & 1 \\
\end{bmatrix}
$$
And got the resulting matrix
$$
\begin{bmatrix}
1 & -1 \\
0 & -2 s-4 \\
0 & 0 \\
0 & 0 \\
\end{bmatrix}
$$
Taking the upper square matrix we get that our gcrd is
$$
gcrd =
\begin{bmatrix}
1 & -1 \\
0 & -2s-4 \\
\end{bmatrix}
$$

\item Find an irreducible right MFD for $H(s)$ by extracting the gcrd of $N(s)$ and $D(s)$.
\newline
To solve $N(s) = N_1(s)R(s)$ we can find the inverse of $R(s)$ and use $N(s)R^{-1}(s) = N_1(s)$
which we know exists since the matrix $\begin{bmatrix} D(s) \\ N(s) \end{bmatrix}$ has full column rank.
The same argument goes for $D_1(s)$
$$
R^{-1}(s) =
\begin{bmatrix}
1 & \frac{1}{-2s-4} \\
0 & \frac{1}{-2s-4} \\
\end{bmatrix}
\implies
N_1(s) =
\begin{bmatrix}
-s & 0 \\
s+1 & -\frac{1}{2} \\
\end{bmatrix}
\quad
D_1(s) =
(s-1)(s+2)
\begin{bmatrix}
s+2 & -\frac{1}{2} \\
0 & -\frac{1}{2} \\
\end{bmatrix}
$$
We can verify this holds true by plugging back into $H(s) = N(s)D(s)^{-1}$ and see that we get the original system.
We can also see that
$$ H(s) = N(s)D(s)^{-1} = N_1(s)R(s)\left(D_1(s)R(s)\right)^{-1} = N_1(s)D_1^{-1}(s) $$
This is our irreducible right MFD for $H(s)$
$$
H(s) =
\begin{bmatrix}
-s & 0 \\
s+1 & -\frac{1}{2} \\
\end{bmatrix}
\left(
(s-1)(s+2)
\begin{bmatrix}
s+2 & -\frac{1}{2} \\
0 & -\frac{1}{2} \\
\end{bmatrix}
\right)^{-1}
$$

\item Determine the poles and zeros of the system based on the irreducible MFD in (b).
\newline
Given a MFD the poles of $H(s)$ are the roots of the determinant of $D(s)$.
The zeros of a square non-singular $H(s)$ as we have are when frequencies when the rank of $N(s)$ drops below its normal rank $det(N(s)) = 0$
\newline
Starting with the poles we get
$$
(s-1)(s+2)
\begin{vmatrix}
s+2 & -\frac{1}{2} \\
0 & -\frac{1}{2} \\
\end{vmatrix}
=
-\frac{1}{2}(s-1)^2(s+2)^3
\implies s = \{-2,-2,-2,1,1\}
$$
We can see that the poles match our previous result in both value and multiplicity.
\newline
For the zeros we get
$$
\begin{vmatrix}
-s & 0 \\
s+1 & -\frac{1}{2} \\
\end{vmatrix}
= \frac{s}{2}
\implies s = 0
$$
Which again matches the zeros for our previous result.
\newline
\fbox{The zeros of our system are $s = 0$ and the poles are $s = \{-2, -2, -2, 1, 1\}$}

\item Find a state-space realization for the MFD in (c)
\newline
Finding the controller form realization from a right MFD we have
$$ A_c = A_C^o - B_C^oD_{hc}^{-1}D_{lc} \quad B_c = B_C^oD_{hc}^{-1} \quad C_c = N_{lc} $$
Where
\begin{itemize}[$\bullet$]
\item $A_C^o$ is block diagonal of ones below the diagonal with their number corresponding to how many derivatives are needed for each input.
\item $B_C^o$ is a column of all zeros except for a 1 corresponding to the start of the block it's associated with.
\item $D(s) = D_{hc}S(s) + D_{lc}(s)\Psi(s)$ where $S(s)$ is the largest degree $s$ in a column and $\Psi(s)$ is the remaining terms required for each column.
\item $N(s) = N_{lc}\Psi(s) $ where $\Psi(s)$ is the same as in the $D(s)$ equation.
\end{itemize}
Looking at $D(s)$ we can see that the first column is of degree $s^3$ while the second is has degree $s^2$.
So our realization will be of degree 5, and our $A_C^o$ and $B_C^o$ matrices are given by
$$
A_C^o =
\begin{bmatrix}
0 & 0 & 0 & 0 & 0 \\
1 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 \\
\end{bmatrix}
\quad
B_C^o =
\begin{bmatrix}
1 & 0 \\
0 & 0 \\
0 & 0 \\
0 & 1 \\
0 & 0 \\
\end{bmatrix}
$$
Our $S(s)$ and $\Psi(s)$ matrices are given by
$$
S(s) =
\begin{bmatrix}
s^3 & 0 \\
0 & s^2 \\
\end{bmatrix}
\quad
\Psi(s) =
\begin{bmatrix}
s^2 & 0 \\
s & 0 \\
1 & 0 \\
0 & s \\
0 & 1 \\
\end{bmatrix}
$$
With this we can find the remaining matrices
$$
D_{hc} =
\begin{bmatrix}
1 & -\frac{1}{2} \\
0 & -\frac{1}{2} \\
\end{bmatrix}
\quad
D_{lc} =
\begin{bmatrix}
3 & 0 & -4 & -\frac{1}{2} & 1 \\
0 & 0 &  0 & -\frac{1}{2} & 1 \\
\end{bmatrix}
\quad
N_{lc} =
\begin{bmatrix}
0 & -1 & 0 & 0 & 0 \\
0 & 1 & 1 & 0 & -\frac{1}{2} \\
\end{bmatrix}
$$
Plugging in and evaluating for $A_c, B_c, C_c$ we get
$$
A_c =
\begin{bmatrix}
-3 & 0 & 4 & 0 & 0 \\
1 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & -1 & 2 \\
0 & 0 & 0 & 1 & 0 \\
\end{bmatrix}
\quad
B_c =
\begin{bmatrix}
1 & -1 \\
0 & 0 \\
0 & 0 \\
0 & -2 \\
0 & 0 \\
\end{bmatrix}
\quad
C_c =
\begin{bmatrix}
0 & -1 & 0 & 0 & 0 \\
0 & 1 & 1 & 0 & -\frac{1}{2} \\
\end{bmatrix}
$$
Putting it all together our system looks like
$$
\begin{bmatrix}
\dddot{x}_1 \\
\ddot{x}_1 \\
\dot{x}_1 \\
\ddot{x}_2 \\
\dot{x}_2 \\
\end{bmatrix}
=
\begin{bmatrix}
-3 & 0 & 4 & 0 & 0 \\
1 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & -1 & 2 \\
0 & 0 & 0 & 1 & 0 \\
\end{bmatrix}
\begin{bmatrix}
\ddot{x}_1 \\
\dot{x}_1 \\
x_1 \\
\dot{x}_2 \\
x_2 \\
\end{bmatrix}
+
\begin{bmatrix}
1 & -1 \\
0 & 0 \\
0 & 0 \\
0 & -2 \\
0 & 0 \\
\end{bmatrix}
\begin{bmatrix}
u_1 \\
u_2 \\
\end{bmatrix}
$$
$$
y(t) =
\begin{bmatrix}
0 & -1 & 0 & 0 & 0 \\
0 & 1 & 1 & 0 & -\frac{1}{2} \\
\end{bmatrix}
\begin{bmatrix}
\ddot{x}_1 \\
\dot{x}_1 \\
x_1 \\
\dot{x}_2 \\
x_2 \\
\end{bmatrix}
$$

\item Is the state-space realization controllable and observable? Is it a minimal realization?
\newline
We know that this system is minimal since "any realization of an MFD with order equal to the degree of the determinant of the denominator matrix will be minimal if and only if the MFD is irreducible."
This is taken from the notes in chapter 4B page 28.
We can still verify it easily with the PBH Test.
\newline
We already have the eigenvalues so we just need to test the PBH controllability matrix for $s = \{-2,1\}$
$$
\begin{bmatrix}
s+3 & 0 & -4 & 0 & 0 & 1 & -1 \\
-1 & s & 0 & 0 & 0 & 0 & 0 \\
0 & -1 & s & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & s+1 & -2 & 0 & -2 \\
0 & 0 & 0 & -1 & s & 0 & 0 \\
\end{bmatrix}
$$
Evaluating and finding the rank for both we see that it does in fact has full rank of $5$ for both values.
\newline
Next the PBH observability matrix
$$
\begin{bmatrix}
s+3 & 0 & -4 & 0 & 0 \\
-1 & s & 0 & 0 & 0 \\
0 & -1 & s & 0 & 0 \\
0 & 0 & 0 & s+1 & -2 \\
0 & 0 & 0 & -1 & s \\
0 & -1 & 0 & 0 & 0 \\
0 & 1 & 1 & 0 & -\frac{1}{2} \\
\end{bmatrix}
$$
Evaluating and finding the rank for both we see that it again has a full rank of $5$ for both values.
\newline
\fbox{The system is both controllable, observable, and therefore minimal realization as we expect from an irreducible MFD}

\item Can you find a similarity transformation which relates the realization in Problem \#1(c) and problem \#2(d)?
If yes, show the results and procedure. If not explain.
\newline
We should be able to as they describe the same system and have the same degree.
To find the transformation we must find a matrix $T$ such that $\{A_2 = T^{-1}A_1T, B_2=T^{-1}B_1, C_2=C_1T\}$
Where $\{ A_1, B_1, C_1 \}$ represents the minimal representation from the first problem and $\{ A_1, B_1, C_1 \}$ from the second.
Matrices are similar if they both have the same diagonalization.
If we diagonailize both matrices
$$ A_1 = S_1JS_1^{-1} \quad A_2 = S_2JS_2^{-1} \implies S_1^{-1}A_1S_1 = J \quad S_2^{-1}A_2S_2 = J$$
Combining we get
$$ S_1^{-1}A_1S_1 = S_2^{-1}A_2S_2  \implies S_2S_1^{-1}A_1 = A_2S_2S_1^{-1}$$
$$ T = S_1^{-1}S_2 \implies A_2 = T^{-1}A_1T$$
So we just need to diagonalize by finding the Jordan decomposition since the matricies have repeated eigenvalues.
$$A_1 \to S_1 =
\begin{bmatrix}
0 & -\frac{1}{2} & -\frac{1}{4} & 1 & 0 \\
-\frac{1}{2} & \frac{1}{4} & \frac{1}{8} & 0 & 1 \\
0 & 1 & 0 & 1 & 0 \\
1 & -\frac{1}{2} & 0 & 0 & 1 \\
0 & -2 & 1 & 1 & 0 \\
\end{bmatrix}
\quad
J =
\begin{bmatrix}
-2 & 0 & 0 & 0 & 0 \\
0 & -2 & 1 & 0 & 0 \\
0 & 0 & -2 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 1 \\
\end{bmatrix}
$$
$$A_2 \to S_2 =
\begin{bmatrix}
0 & -4 & 4 & 0 & -1 \\
0 & 2 & -1 & 0 & -1 \\
0 & -1 & 0 & 0 & -1 \\
2 & 0 & 0 & -1 & 0 \\
-1 & 0 & 0 & -1 & 0 \\
\end{bmatrix}
\quad
J =
\begin{bmatrix}
-2 & 0 & 0 & 0 & 0 \\
0 & -2 & 1 & 0 & 0 \\
0 & 0 & -2 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 1 \\
\end{bmatrix}
$$
Which have the same Jordan decomposition as we expect.
Computing $T$ we get
$$
T =
S_2S_1^{-1}
=
\begin{bmatrix}
-\frac{11}{3} & -\frac{2}{3} & \frac{1}{2} & -\frac{1}{3} & \frac{19}{6} \\
\frac{1}{3} & -\frac{2}{3} & \frac{1}{2} & -\frac{1}{3} & -\frac{5}{6} \\
\frac{1}{3} & -\frac{2}{3} & -\frac{1}{2} & -\frac{1}{3} & \frac{1}{6} \\
-\frac{10}{9} & -\frac{4}{3} & \frac{2}{9} & \frac{4}{3} & -\frac{1}{9} \\
-\frac{1}{9} & \frac{2}{3} & -\frac{7}{9} & -\frac{2}{3} & -\frac{1}{9} \\
\end{bmatrix}
$$
Now we can verify that this similarity transformation does convert between the two realizations by performing the computation and checking.
This works for the $A$ matrix, but the transformation doesn't work for the $B$ and $C$ matrix.
Since the similarity Jordan decomposition matrices are unique there is only one transformation matrix for the $A$ matrix.
We can also see that if we apply the Jordan similarity transformation that $\bar{B}_1 \neq \bar{B}_2$ and $\bar{C}_1 \neq \bar{C}_2$ which also shows that there is not a similarity transformation.
\fbox{There is not a similarity transformation as our state space matrices $A$ are not diagonalizable}
\end{enumerate}

\newpage
\section*{Problem 3}
Consider the following system,
\begin{figure}[!htb]
\centering
\tikzset{
    block/.style = {draw, fill=white, rectangle, minimum height=1em, minimum width=1em},
    tmp/.style  = {coordinate, node distance=1cm},
    sum/.style= {draw, fill=white, circle, node distance=1cm},
    input/.style = {coordinate, node distance=1cm},
    output/.style= {coordinate, node distance=2cm},
    pinstyle/.style = {pin edge={to-,thin,black}},
}
\begin{tikzpicture}[auto, node distance=2cm,>=latex']
\node [sum, name=inputSum ](inputSum){};
\node [block, right of=inputSum](controller){$K(s)$};
\node [block, right of=controller](transferFunc){$G(s)$};
\node [tmp, right of=transferFunc](tmpOutput) {};
\node [output, right of=tmpOutput](output) {};
\node [tmp, below of=transferFunc](belowTF) {};

\draw [->] (inputSum) --  (controller) {};
\draw [->] (controller) -- node[below]{$u(t)$} (transferFunc) {};
\draw [-] (transferFunc) -- (tmpOutput) {};
\draw [->] (tmpOutput) -- node[above]{$Y(s)$} node[below]{$y(t)$} (output) {};
\draw [-] (tmpOutput) |- (belowTF) {};
\draw [->] (belowTF) -| (inputSum) {};
\end{tikzpicture}
\end{figure}
where
$$ G(s) = \frac{s-2}{s^2-4s} $$
In the following you will design an observer-based controller $K(s)$ so that the closed-loop system is stable.
\begin{enumerate}[(a)]
\item Find a state space representation of $G(s)$.
\newline

\item Use the Riccati-equation approach to determine an observer based controller $K_1(s)$ such that the closed loop system is stable.
\newline

\item Find a state space representation of the closed loop system with $y(t)$ as the output.
\newline

\item Verify that the closed loop system poles are the regulator poles together with the observer poles.
\newline

\item Plot the state response for $x(t)$ and $u(t)$ due to the initial conditions $y(0)$, and $\dot{y}(0)$.
\newline

\item Repeat (b), (c), (d), and (e) with a different observer-based controller $K_2(s)$, which is obtained by choosing different weighting matrices in the Riccati equations.
\newline

\item Comment on how the weighting matrices in the Riccati equation and the pole location of the closed-loop system affect the closed loop system performance.
\newline

\end{enumerate}

\end{document}
